# bio-data-analysis

Draft notes for a grad course in biological data analysis and best practices

## General idea

Provide first year grad students with an overview of computing concepts and tools that are frequently used in the biological sciences. Along the way, build community among students and expose them to some of the research going on in the department. And also along the way, illustrate how the material presented in class is applied to everyday research projects.

Wide rather than deep? Or generally wide, but deep in a few selected areas? Expose students to many different tools they might want to leverage, so they know where to dig deeper when they need it. But with enough "meat" to be immediately useful -- to culture good data practices in terms of data management, data archiving, reproducible data analysis, skills to deal with high throughput data. Draw on example datasets generated by biograds, illustrative of challenges/opportunities, and reflecting the diversity of research projects in the department.

Don't assume any prior programming experience. Motivate with domain-based examples. Learn from Carpentries model. 

## Structure ideas

* Weekly lecture + hands-on: 2-hour classroom session

* Weekly data lunches: grad students from dept present overview of data types and data tools they use in their own research; reinforcing concepts from class and demonstrating relevance; TA moderates

* Faculty talks: have a variety of faculty talk about data analysis methods, tools, reproducicibility, data archiving, and other challenges they face in their research program; then use their data sets as examples. These talks would be during normal lecture time.

* One semester or two? -- leaning towards 2 semesters to allow time for students to absorb concepts, work through homework problems 

* Integrating data visualization -- beyond the mechanics of how to make a figure, but what makes for good figure.  Drawing on Greg's prior course.

## Draft high-level outline (* = more extensive modules)

0 Orientation
* Course goals and mechanics
* Working with data: clean, reformat, explore, analyze, model, present, share
* Data science principles: FAIR, open-source, non-proprietary, etc. 
* Help installing necessary software, familiarization with VSCode 

1* Unix
* First steps: intro to Unix, UNIX file system, 
* Working with remote computers/file systems: ssh, sftp, curl
* Unix basics: navigating the file system, files and directories
* Working with files: creation, viewing, comparison, permissions
* Data wrangling tools: cat, sort, uniq, tr, cut, join
* Scripting
* Regular expressions: grep
* Unix tools deeper dive: find, sed, awk 

2* Python
* First steps: familiarization with Jupyter (or Quarto?), intro to Python
* Basics: data structues, flow control, etc.
* Data wranging in Python: NumPy, Pandas (but not data viz and Seaborn?)
* What and how much more is appropriate here? 

3 Programming best practices
* It starts with the little things: commenting, variable names, choosing the right data structures, etc.
* Debugging 
* Testing and validation
* Version control: Github (using Python as an example)

4* R
* First steps: familiarization with R Studio environment, intro to R
* Basics: importing/exporting, base plots
* Markdown in R (or Quarto?)
* Data wrangling in R: tibbles, Tidyr, Dplyr
* Advanced data viz in R: ggplot2
* Applications for biology: geospatial and genomic data 
* Principles of effective and ethical data visualization
* Tour of useful R packages for biologists

5 SQL
* Relational database concepts
* Orientation to SQLite
* The all-important SELECT statement
* Deeper dive into queries: nested queries, aliases, views, joins 
* Leveraging Sql from Python and R

6 Data science best practices
* Being thoughtful: DMP
* Data integrity: secure storage, checksums, metadata, persistent identifiers
* Working with data: organizing files, file naming conventions, intermediate data products
* Choosing the right tool for the job: these and other languages
* Reproducibility throughout the workflow
* Exposing data: repositories, reusability, discoverability


## Topic ideas: to be integrated into above structure 

* The big picture
    - data lifecycle
    - reproducibility, reusibility, discoverability
    - DMP as a living document

* File system organization across major OS's
    - interacting with remote file systems
    - transferring large files, archiving datasets

* Introduction to working at the command line 

* Regular expressions -- grep 
    - example application: finding TF binding motifs

* Data concepts
    - data and metadata
    - special data types: date, time, geolocation, etc. 
    - special values: null, NaN
    - how to deal with outliers? when is imputation appropriate?

* Working with tabular data at the command line -- awk, sort, join
    - example biological application: working with GFF annotation data

* Shell scripting -- bash
    - example application: aligning reads to a reference genome

* Data wrangling in R -- dplyr and tidyr

* Visualization in R -- ggplot2

* A tour of popular biology related libraries for R

* Introduction to Python
    - Jupyter notebooks: introduce along with starting Python?
    - example application: reorganizing and cleaning a flat file downloaded from a repository 

* The Python Scientific Stack -- numpy, scipy, pandas, matplotlib, jupyter notebooks

* Simulation in Python
    - example application: Boolean network modeling of a gene regulatory network 
        - [Orlando et al. 2008]() -- from Steve Haase's lab

* Working with sequence data in Python
    - example application: working with FASTA files, multiple sequence alignments, and phylogenetic trees

* Version control  -- git

* Data archiving and data management

* Introduction to databases -- sqlite (or postgresql?)
    - example biological application: building a multi-table relational database of ecological data
        * [Ames et al.](https://esajournals.onlinelibrary.wiley.com/doi/full/10.1002/ecy.1886) -- from Justin Wright's lab
    - sql queries
